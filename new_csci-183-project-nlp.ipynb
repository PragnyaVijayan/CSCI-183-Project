{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Inspiration taken from: https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/"]},{"cell_type":"code","execution_count":null,"id":"cda66b59","metadata":{"execution":{"iopub.execute_input":"2024-03-21T06:41:44.751233Z","iopub.status.busy":"2024-03-21T06:41:44.750662Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import string\n","import matplotlib.pyplot as plt\n","import math\n","\n","\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","import transformers\n","from transformers import BertTokenizer, BertModel\n","import tensorflow as tf\n","\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import BertTokenizer, BertModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import Dataset, TensorDataset"]},{"cell_type":"code","execution_count":null,"id":"72c3031f","metadata":{"trusted":true},"outputs":[],"source":["pd.reset_option('display.max_rows')"]},{"cell_type":"code","execution_count":null,"id":"621a42f3","metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/spell-checked-df-csv/spell_checked_df.csv')\n","df"]},{"cell_type":"code","execution_count":null,"id":"6df8335c","metadata":{"trusted":true},"outputs":[],"source":["def remove_words(commentary):\n","    \n","    string_value = commentary.replace('[', '').replace(']', '').replace(';', ',')\n","    string_value = string_value.replace(\"'\", \"\")\n","    list_value = string_value.split(',')\n","    \n","    words_to_remove = [\"sri\", \"lanka\", \"hong\", \"kong\", \"rajapaksa\"]\n","    return [word.strip() for word in list_value if word.strip() not in words_to_remove]"]},{"cell_type":"code","execution_count":null,"id":"cb98f81b","metadata":{"trusted":true},"outputs":[],"source":["df['new_preprocessed_commentary'] = df['preprocessed_commentary'].apply(lambda x: remove_words(x))\n","df"]},{"cell_type":"code","execution_count":null,"id":"1d9f8bdd","metadata":{"trusted":true},"outputs":[],"source":["def get_batsmen_bowler_names(df):\n","    bowlers = list(df['bowler'].unique())\n","    batsmen = list(df['batsman'].unique())\n","    players = bowlers + batsmen\n","    players = [name.lower().split()[0] for name in players]\n","    \n","    return players\n","\n","def remove_players(text, players):\n","    keywords = ['bhuvneshwar', 'suryakumar', 'asalanka', 'silva', 'khushdil', 'kohl', 'nissanka',\n","                'madushanka', 'iftikhar', 'hasnain', 'hasaranga', 'karunaratn', 'nawab']\n","    \n","    players = players + keywords\n","    return [word for word in text if word not in players]\n"]},{"cell_type":"code","execution_count":null,"id":"095e6266","metadata":{"trusted":true},"outputs":[],"source":["players = get_batsmen_bowler_names(df)\n","df['new_preprocessed_commentary_players_removed'] = df['new_preprocessed_commentary'].apply(lambda x: remove_players(x, players))\n","df"]},{"cell_type":"code","execution_count":null,"id":"a889f8dc","metadata":{"trusted":true},"outputs":[],"source":["#Initialize the 'bowling_team' column with 'Other'\n","df['bowling_team'] = 'Other'\n","\n","#Iterate over the range of matches\n","for match_no in range(1, 14):\n","    # Extract the batting teams for the current match\n","    team1 = df[(df['match_no'] == match_no) & (df['batting_side'] == 1)]['batting_team'].iloc[0]\n","    team2 = df[(df['match_no'] == match_no) & (df['batting_side'] == 2)]['batting_team'].iloc[0]\n","\n","#Assign the opponent team as the bowling team for each occurrence of a team in the batting team column\n","    df.loc[(df['match_no'] == match_no) & (df['batting_team'] == team1), 'bowling_team'] = team2\n","    df.loc[(df['match_no'] == match_no) & (df['batting_team'] == team2), 'bowling_team'] = team1\n","\n","#Print the DataFrame to verify\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"c761ae89","metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["df['wicket_fall'] = df['runs'].str.contains('\\d[w]', regex=True)\n","print(df['wicket_fall'].value_counts())"]},{"cell_type":"code","execution_count":null,"id":"c324880c","metadata":{"trusted":true},"outputs":[],"source":["def group_overs(over):\n","    integer_part = math.floor(over)    \n","    decimal_part = over - integer_part\n","    \n","          \n","    grouped_over = integer_part + 1\n","    \n","    return grouped_over"]},{"cell_type":"code","execution_count":null,"id":"ab0f0d30","metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["df['grouped_overs'] = df['overs'].apply(group_overs)\n","df.iloc[120]"]},{"cell_type":"code","execution_count":null,"id":"296686f3","metadata":{"trusted":true},"outputs":[],"source":["df.groupby(by=\"grouped_overs\")['wicket_fall'].sum()"]},{"cell_type":"code","execution_count":null,"id":"2735d684","metadata":{"trusted":true},"outputs":[],"source":["df_over = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"id":"7b086a64","metadata":{"trusted":true},"outputs":[],"source":["\n","df_sorted = df.sort_values(by=[\"match_no\", \"batting_side\", \"grouped_overs\"])\n","\n","grouped = df_sorted.groupby([\"match_no\", \"batting_side\", \"grouped_overs\"])\n","\n","# Initialize lists to store over-by-over data\n","over_numbers = []\n","total_runs = []\n","wicket_fall = []\n","match_numbers = []\n","batting_sides = []\n","batting_teams = []\n","bowling_teams = []\n","commentary = []\n","comment = []\n","bowlers = []\n","batsmen = []\n","\n","# Iterate over groups\n","for group_key, group_df in grouped:\n","    \n","    match_no, batting_side, over = group_key\n","    runs_in_over = group_df[\"numerical_runs\"].sum()\n","    wicket = 1 if group_df[\"wicket_fall\"].any() else 0\n","    batting_team = \" \".join(group_df['batting_team'].unique())\n","    bowling_team = \" \".join(group_df['bowling_team'].unique())\n","    bowler = \" \".join(group_df['bowler'].unique())\n","    batsman = \" ,\".join(group_df['batsman'].unique())\n","    \n","    comment_list = group_df[\"new_preprocessed_commentary_players_removed\"].tolist()\n","    comment = []  # Initialize comment as an empty list\n","    [comment.extend(sublist) for sublist in comment_list]\n","    \n","    over_numbers.append(over)\n","    total_runs.append(runs_in_over)\n","    wicket_fall.append(wicket)\n","    match_numbers.append(match_no)\n","    batting_sides.append(batting_side)\n","    commentary.append(comment)\n","    bowling_teams.append(bowling_team)\n","    batting_teams.append(batting_team)\n","    bowlers.append(bowler)\n","    batsmen.append(batsman)\n","\n","# Create a new DataFrame for over-by-over summary\n","df_over = pd.DataFrame({\n","    \"match_no\": match_numbers,\n","    \"batting_side\": batting_sides,\n","    \"batting_team\": batting_teams,\n","    \"bowling_team\": bowling_teams,\n","    \"over\": over_numbers,\n","    \"total_runs\": total_runs,\n","    \"wicket_fall\": wicket_fall,\n","    \"commentary\": commentary,\n","    \"bowlers\": bowlers,\n","    \"batsmen\": batsmen\n","})\n","\n","# Reset index\n","df_over.reset_index(drop=True, inplace=True)\n","\n","df_over"]},{"cell_type":"code","execution_count":null,"id":"e25306ff","metadata":{"trusted":true},"outputs":[],"source":["def get_winner(row):\n","    winners = ['AFG', 'IND', 'AFG', 'IND', 'SL', 'PAK', 'SL', 'PAK', 'SL', 'PAK', 'IND', 'SL', 'SL']\n","    index = row['match_no'] - 1  # Subtract 1 to convert match number to zero-based index\n","    if 0 <= index < len(winners):\n","        return winners[index]\n","    else:\n","        return 'Unknown'\n","\n","df_over['winner'] = df_over.apply(get_winner, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"5e3b2cc4","metadata":{"trusted":true},"outputs":[],"source":["def who_won(row):\n","    if row['batting_team'] == row['winner']:\n","        return 1\n","    elif row['bowling_team'] == row['winner']:\n","        return 0\n","    \n","df_over['winner_label'] = df_over.apply(who_won, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"25d516e6","metadata":{"trusted":true},"outputs":[],"source":["df_over['winner'].value_counts()"]},{"cell_type":"code","execution_count":null,"id":"d1b2f6d2","metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Group by batting and bowling team and aggregate total runs\n","df_grouped = df_over.groupby(['match_no']).agg({'total_runs': 'sum', 'over': 'count'}).reset_index()\n","df_grouped.rename(columns={'over': 'total_overs'}, inplace=True)\n","\n","# Separate features and target variable\n","X = df_grouped[['total_overs']]\n","y = df_grouped['total_runs']\n","\n","# Feature scaling for numerical features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train-test split for model evaluation\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n","\n","# Define and train Linear Regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions on testing set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate model performance using Mean Squared Error and R-squared\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r2)\n"]},{"cell_type":"code","execution_count":null,"id":"1810ee21","metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Create index array to keep track of the indices\n","indices = np.arange(len(X_test))\n","\n","# Plotting each match separately\n","plt.figure(figsize=(15, 10))\n","\n","# Iterate through each match\n","for i in range(13):\n","    match_indices = indices[X_test[:,0] == i]  # Get indices of data points for the current match\n","    print(f\"Match {i+1} indices:\", match_indices)\n","    print(f\"Match {i+1} X_test values:\", X_test[match_indices])\n","    plt.subplot(3, 5, i + 1)\n","    plt.scatter(np.arange(len(match_indices)), y_test.iloc[match_indices], color='blue', label='Actual')\n","    plt.plot(np.arange(len(match_indices)), y_pred[match_indices], color='red', linestyle='--', label='Predicted')\n","    plt.title(f'Match {i+1}')\n","    plt.xlabel('Over')\n","    plt.ylabel('Total Runs')\n","    plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"e1de5d05","metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error, r2_score\n","import matplotlib.pyplot as plt\n","\n","# Separate features and target variable\n","X = df_over[['bowling_team', 'batting_team', 'over']]\n","y = df_over['total_runs']\n","\n","# Convert 'over' column to string type\n","X['over'] = X['over'].astype(str)\n","\n","# Encode categorical features (bowling_team, batting_team)\n","encoder = OneHotEncoder(sparse=False)\n","X_encoded = encoder.fit_transform(X[['bowling_team', 'batting_team']])\n","\n","# Combine encoded features with numerical feature (over)\n","X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(['bowling_team', 'batting_team']))\n","X = pd.concat([X[['over']], X_encoded_df], axis=1)\n","\n","# Feature scaling for numerical features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X.drop(columns=['over']))\n","\n","# Train-test split for model evaluation\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n","\n","# Define and train SVR model\n","model = SVR(kernel='rbf', C=100, epsilon=0.1)\n","model.fit(X_train, y_train)\n","\n","# Make predictions on testing set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate model performance using Mean Squared Error and R-squared\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","print(\"Mean Squared Error:\", mse)\n","print(\"R-squared:\", r2)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"97f6cf9d","metadata":{"trusted":true},"outputs":[],"source":["# Create a line plot to compare actual and predicted total runs\n","plt.plot(np.arange(len(y_test)), y_test, label='Actual Total Runs')\n","plt.plot(np.arange(len(y_test)), y_pred, label='Predicted Total Runs')\n","plt.xlabel(\"Index of Test Data\")\n","plt.ylabel(\"Total Runs\")\n","plt.title(\"Actual vs Predicted Total Runs\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"59826382","metadata":{"trusted":true},"outputs":[],"source":["# Perform groupby operation and calculate the sum of 'wicket_fall' for each 'overs'\n","grouped_data = df_over.groupby(by=\"over\")['wicket_fall'].sum()\n","\n","# Create a histogram using pandas plotting functionality\n","grouped_data.plot(kind='bar', figsize=(10, 6), color='skyblue')\n","\n","# Customize the plot\n","plt.title('Wickets Fallen per Over')\n","plt.xlabel('Overs')\n","plt.ylabel('Number of Wickets')\n","plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n","plt.gca().yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n","\n","\n","# Display the plot\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# NLP"]},{"cell_type":"code","execution_count":null,"id":"e40d37fa","metadata":{"trusted":true},"outputs":[],"source":["# Convert Series objects to NumPy arrays\n","numerical_runs_array = df_over['total_runs'].values.reshape(-1, 1)\n","grouped_overs_array = df_over['over'].values.reshape(-1, 1)\n","\n","# Normalize the arrays to the range [0, 1]\n","scaler = preprocessing.MinMaxScaler()\n","numerical_runs_normalized = scaler.fit_transform(numerical_runs_array)\n","grouped_overs_normalized = scaler.fit_transform(grouped_overs_array)\n","\n","# Assign normalized values back to DataFrame\n","df_over['runs_normalized'] = numerical_runs_normalized\n","df_over['overs_normalized'] = grouped_overs_normalized\n","\n","df_over"]},{"cell_type":"code","execution_count":null,"id":"374374e3","metadata":{"trusted":true},"outputs":[],"source":["# Load the pre-trained BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"id":"0a3428c7","metadata":{"trusted":true},"outputs":[],"source":["over_data = [text for text in df_over['overs_normalized']]\n","runs_data = [text for text in df_over['runs_normalized']]"]},{"cell_type":"code","execution_count":null,"id":"9758634d","metadata":{"trusted":true},"outputs":[],"source":["sen_w_feats = []\n","labels = []\n","\n","for index, row in df_over.iterrows():   \n","    combined = \"\"\n","    \n","    combined += \"In the Asia Cup Match between {:} and {:} in over {:}, \" \\\n","                \"{:} scored {:} runs against bowler {:}. There were {:} wickets. \"\\\n","                \"The live commentary was: {:}\".format(row[\"batting_team\"], \n","                                                       row[\"bowling_team\"], \n","                                                       row[\"over\"],\n","                                                       row[\"batsmen\"], \n","                                                       row['total_runs'],\n","                                                       row[\"bowlers\"],\n","                                                       row['wicket_fall'],\n","                                                       row['commentary'])\n","    \n","    # Add the combined text to the list.\n","    sen_w_feats.append(combined)\n","\n","    # Record the sample's label.\n","    labels.append(row['winner_label'])\n","\n","print('  DONE.')\n","\n","print('Dataset contains {:,} samples.'.format(len(sen_w_feats)))"]},{"cell_type":"code","execution_count":null,"id":"e09f2ad0","metadata":{"trusted":true},"outputs":[],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"id":"0ececdd4","metadata":{"trusted":true},"outputs":[],"source":["from transformers import BertForSequenceClassification\n","\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", \n","    num_labels = 2, #2 for binary classification.\n",")\n","\n","# desc = model.cuda()"]},{"cell_type":"code","execution_count":null,"id":"6152a402","metadata":{"trusted":true},"outputs":[],"source":["batch_size = 1\n","learning_rate = 1e-5\n","epochs = 2"]},{"cell_type":"code","execution_count":null,"id":"327dada7","metadata":{"trusted":true},"outputs":[],"source":["max_len = 0\n","\n","for sent in sen_w_feats:\n","\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"]},{"cell_type":"code","execution_count":null,"id":"804c9484","metadata":{"trusted":true},"outputs":[],"source":["#Set max length = 512\n","max_len=512"]},{"cell_type":"code","execution_count":null,"id":"6f6ba879","metadata":{"trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","print('Encoding all reviews in the dataset...')\n","\n","for sent in sen_w_feats:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,\n","                        truncation = True,\n","                        padding = 'max_length',\n","                        return_attention_mask = True,   \n","                        return_tensors = 'pt',  \n","                )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","print('DONE.')"]},{"cell_type":"code","execution_count":null,"id":"5976ea15","metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","# Split data into training and test sets\n","train_input_ids, test_input_ids, train_attention_masks, test_attention_masks, train_labels, test_labels = train_test_split(\n","    input_ids, attention_masks, labels, test_size=0.2, random_state=42)\n","\n","# Further split the training set into training and validation sets\n","train_input_ids, val_input_ids, train_attention_masks, val_attention_masks, train_labels, val_labels = train_test_split(\n","    train_input_ids, train_attention_masks, train_labels, test_size=0.2, random_state=42)\n","\n","# Create TensorDatasets for each split\n","train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"]},{"cell_type":"code","execution_count":null,"id":"ebd16683","metadata":{"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size\n","        )\n","\n","validation_dataloader = DataLoader(\n","            val_dataset,\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )"]},{"cell_type":"code","execution_count":null,"id":"96f1931a","metadata":{"trusted":true},"outputs":[],"source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = learning_rate, \n","                  eps = 1e-8 \n","                )"]},{"cell_type":"code","execution_count":null,"id":"4d524fa4","metadata":{"trusted":true},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"id":"7c2cde13","metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"id":"94d075c6","metadata":{"trusted":true},"outputs":[],"source":["import random\n","import time\n","import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# Set the seed for reproducibility\n","seed_val = 18\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","#training_stats = []\n","\n","#scaler = torch.cuda.amp.GradScaler(init_scale=2**8)\n","\n","for epoch_i in range(epochs):\n","    # Start tqdm progress bar for training\n","    with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch_i + 1}/{epochs}', unit='batch') as pbar:\n","        \n","        total_train_loss = 0\n","        \n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Put the model into training mode.\n","        model.train()\n","\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Clear any previously calculated gradients.\n","            model.zero_grad()\n","\n","            with torch.cuda.amp.autocast():\n","                result = model(b_input_ids, \n","                             token_type_ids = None,\n","                             attention_mask=b_input_mask,\n","                             labels=b_labels,\n","                             return_dict=True)\n","\n","                loss = result.loss\n","                logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches.\n","            total_train_loss += loss.item()\n","\n","            # Perform a backward pass to calculate the gradients.\n","            scaler.scale(loss).backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            #scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","#             scaler.step(optimizer)\n","#             scaler.update()\n","\n","            optimizer.step()\n","\n","            # Update the learning rate.\n","            scheduler.step()\n","\n","            # Release GPU memory\n","            torch.cuda.empty_cache()\n","\n","            # Update tqdm progress bar\n","            pbar.update(1)\n","            pbar.set_postfix({'loss': loss.item()})\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","        # Measure how long this epoch took.\n","        training_time = time.time() - t0\n","\n","        print(\"\")\n","        print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"Training epoch took: {:.2f} seconds\".format(training_time))"]},{"cell_type":"code","execution_count":null,"id":"f187c55f","metadata":{"trusted":true},"outputs":[],"source":["# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":null,"id":"3375025c","metadata":{},"outputs":[],"source":["#Model needs to be run on validation_dataloader but the model training with train_dataset not completing due to GPU resource issues \n","# even after reducing batch size and attempting to use a scaler"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4638632,"sourceId":7898806,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}
